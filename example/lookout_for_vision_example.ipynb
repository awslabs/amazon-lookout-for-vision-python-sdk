{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Lookout for Vision Python SDK\n",
    "\n",
    "In this notebook we will walk you through the Amazon Lookout for Vision Python SDK. It gives you a programmatic way of interacting with this service and adds a lot of helper functions that complement the service, like:\n",
    "\n",
    "* create manifest file\n",
    "* push manifest file to S3\n",
    "* check image sizes if they comply with the service\n",
    "* check image shapes if you need to rescale images\n",
    "* rescale images based on optimal shape\n",
    "* upload images to S3 in the appropriate structure\n",
    "\n",
    "**Requirements**\n",
    "\n",
    "Have your images on this local instance. The bad images should be stored in a folder called *bad*, and the good images in a folder named *good*. Also note that the only formats allowed are: jpeg, jpg and png. The following url describes the quotas/limitation of images for training and validation --> https://docs.aws.amazon.com/lookout-for-vision/latest/developer-guide/limits.html \n",
    "\n",
    "## Training a Model\n",
    "\n",
    "First let's set some general variables that you need:\n",
    "\n",
    "* input_bucket: the S3 bucket that contains your images for training a model\n",
    "* project_name: the unique name of the Amazon Lookout for Vision project\n",
    "* model_version: the model version you want to deploy (note: when starting fresh \"1\" is the default)\n",
    "* output_bucket: a bucket where your model and inference results are stored (can be same as input_bucket)\n",
    "* input_prefix: if you run inference out of S3 this is the key of the image(s) you want to predict\n",
    "* output_prefix: this is the S3 key where your prediction(s) will be saved to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training & Inference\n",
    "input_bucket = \"YOUR_S3_BUCKET_FOR_TRAINING\"\n",
    "project_name = \"YOUR_PROJECT_NAME\"\n",
    "model_version = \"1\" # leave this as one if you start right at the beginning\n",
    "# Inference\n",
    "output_bucket = \"YOUR_S3_BUCKET_FOR_INFERENCE\" # can be same as input_bucket\n",
    "input_prefix = \"YOUR_KEY_TO_FILES_TO_PREDICT/\" # used in batch_predict\n",
    "output_prefix = \"YOUR_KEY_TO_SAVE_FILES_AFTER_PREDICTION/\" # used in batch_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the SDK using pip\n",
    "# !pip install lookoutvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the libraries needed to get started:\n",
    "from lookoutvision.image import Image\n",
    "from lookoutvision.manifest import Manifest\n",
    "from lookoutvision.lookoutvision import LookoutForVision\n",
    "from lookoutvision.metrics import Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the necessary classes:\n",
    "\n",
    "* Image to interact with your local images\n",
    "* Manifest to generate and push manifest files\n",
    "* Metrics to view and compare Model metrics\n",
    "* LookoutForVision as the main class to interact with the service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mft = Manifest(\n",
    "    bucket=input_bucket,\n",
    "    s3_path=\"{}/\".format(project_name),\n",
    "    datasets=[\"training\", \"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l4v = LookoutForVision(project_name=project_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "met = Metrics(project_name=project_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If project does not exist: create it\n",
    "p = l4v.create_project()\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if your local images comply with the service\n",
    "sizes = img.check_image_sizes(verbose=False)\n",
    "print(sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all image shapes are the same\n",
    "shapes = img.check_image_shapes(verbose=True)\n",
    "print(shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If not: rescale them\n",
    "# Note: you don't need to specify a prefix. If you do a new folder is generated for you being named\n",
    "# rescaled_good and rescaled_bad. Without prefix your original images will be overwritten\n",
    "resc = img.rescale(prefix=\"rescaled_\")\n",
    "print(resc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check again in rescaled folder (if you created it)\n",
    "sizes = img.check_image_sizes(prefix=\"rescaled_\", verbose=False)\n",
    "print(sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check again in rescaled folder (if you created it)\n",
    "shapes = img.check_image_shapes(prefix=\"rescaled_\", verbose=True)\n",
    "print(shapes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you prepared your images, have them all in the same shape and they comply with the service's rules you can upload them to your S3 bucket. The Image() class will upload appropriately so you don't need to care about structure anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.upload_from_local(\n",
    "    bucket=input_bucket,\n",
    "    s3_path=\"{}/\".format(project_name),\n",
    "    train_and_test=True,\n",
    "    test_split=0.2,\n",
    "    prefix=\"rescaled_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that your images are saved to S3, you can use the Manifest() class to generate a manifest file for you and push it to the same S3 location in which your image folders are. Lookout for Vision will pick these manifest files up and create datasets accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mft_resp = mft.push_manifests()\n",
    "print(mft_resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the manifest files in S3 create your Lookout for Vision datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsets = l4v.create_datasets(mft_resp, wait=True)\n",
    "print(dsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l4v.fit(\n",
    "    output_bucket=output_bucket,\n",
    "    model_prefix=\"mymodel_\",\n",
    "    wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And final deploy it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l4v.deploy(\n",
    "    model_version=model_version,\n",
    "    wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Model Metrics\n",
    "\n",
    "If you want to check the metrics of your model(s) you can use the *Metrics* class in two different flavors:\n",
    "\n",
    "* Either display the metrics for one model\n",
    "* or display the metrics for all models of the same project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One model\n",
    "met.describe_model(model_version=model_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All models of the same project\n",
    "met.describe_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Batch Transform feature  enables you to run predictions on datasets stored in Amazon S3/local.\n",
    "Batch transform job would run inferences on your batch dataset and store your inference results in S3/local accordingly\n",
    "\n",
    "For batch prediction where your data/images are in s3 ,please provide below information as input to the function.\n",
    "  1. model_version=Either you put your model version or by default it will take model version as 1\n",
    "  2. input_bucket=Input bucket name where your input images ( which are required to be predicted normal/anomalous) are there.\n",
    "  3. input_prefix = Folder name/Key name (if applicable)  for the s3 path where input images are. In case you have this please make sure that you put a forward slash (\"/\") at the end as mentioned in the example.\n",
    "  4. output_bucket = Output bucket name where your prediction results would be stored in json file. Please note that output json file's name would be image_name.json\n",
    "  5. output_prefix = Folder name/Key name (if applicable)  for the s3 path where output predicted files would be>In case you have this please make sure that you put a forward slash (\"/\") at the end as mentioned in the example.\n",
    "  6. content_type=\"image/jpeg\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l4v.batch_predict(\n",
    "    model_version=model_version,\n",
    "    input_bucket=input_bucket,\n",
    "    input_prefix=input_prefix,\n",
    "    output_bucket =output_bucket,\n",
    "    output_prefix=output_prefix,\n",
    "    content_type=\"image/jpeg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For batch prediction where your data/images are in local ,please provide below information as input to the function.\n",
    "\n",
    "1. model_version=Either you put your model version or by default it will take model version as '1'\n",
    "2. local_path= Local path where your input images ( which are required to be predicted normal/anomalous) are there.\n",
    "3. content_type=\"image/jpeg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l4v._batch_predict_local(\n",
    "    local_path='/your/local/path',\n",
    "    model_version=model_version,\n",
    "    content_type=\"image/jpeg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To predict Real-time , call the predict method with below inputs. You can either predict from S3 object OR local images\n",
    " 1. model_version=Either you put your model version or by default it will take model version as '1', \n",
    " 2. local_file=Local path where your input image ( which is required to be predicted normal/anomalous) is there.,\n",
    " 3. bucket=Input bucket name where your input image ( which is required to be predicted normal/anomalous) is there, \n",
    " 4. key=Key for the image (it should contain the exact file name as mentioned in the example below), \n",
    " 5. content_type=\"image/jpeg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When your image is in local path. Please change your local file path with your local directory and file name\n",
    "l4v.predict(local_file=\"your/local/bad/file.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When your image is in local path. Please change your local file path with your local directory and file name\n",
    "l4v.predict(local_file=\"your/local/good/file.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When your image is in s3. Please change your s3 bucket with key and file name\n",
    "l4v.predict(\n",
    "    bucket=input_bucket,\n",
    "    key='my/key/to/the/file.jpeg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To retrain the model of the same project, you need to follow the steps.\n",
    "\n",
    "1. Create new/updated manifest file with new images\n",
    "\n",
    "2. Update the existing datasets ( train and test both)\n",
    "\n",
    "3. Train a new version of model with updated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the buckets for the latest input images which need to be trained  and then initialize the Manifest method with the\n",
    "## In case you have new/updated images in the same bucket you may avoid these steps.\n",
    "mft_retrain = Manifest(\n",
    "    bucket=input_bucket,\n",
    "    datasets=[\"training\", \"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case you have new/updated images for retraining in local, you can use the below method to import the same to s3 \n",
    "img.upload_from_local(\n",
    "    bucket=input_bucket,\n",
    "    train_and_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create the manifest file with new dataset\n",
    "mft_resp_new = mft_retrain.push_manifests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update datasets with new manifest file\n",
    "l4v.update_datasets(mft_resp_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start creation of new model training. This time it will take updated dataset.\n",
    "l4v.fit(output_bucket=input_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop the model after you are done.\n",
    "If you dont provide any model version by default it will stop model version 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When you dont mention the model version \n",
    "l4v.stop_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When you mention the specific model version \n",
    "new_model_version = \"2\"\n",
    "l4v.stop_model(model_version=new_model_version)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
